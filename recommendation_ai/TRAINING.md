# ðŸŒ¾ Training Guide - Crop Recommendation AI

Complete guide to train your own crop recommendation model using a CSV dataset.

## Quick Start

```bash
# 1. Prepare your CSV file
# Required columns: N, P, K, temperature, humidity, pH, rainfall, label

# 2. Train the model
python train.py --csv data/your_crop_data.csv --model models/my_model.pkl

# 3. Use the trained model
python -c "from crop_recommender import CropRecommender
recommender = CropRecommender(model_path='models/my_model.pkl')
result = recommender.recommend(90, 42, 43, 23, 75, 6.5, 200)
print(result)"
```

## CSV Format

Your training CSV must have exactly these columns:

```csv
N,P,K,temperature,humidity,pH,rainfall,label
90,42,43,20.88,81,6.02,202.9,rice
85,58,41,21.77,80.32,7.04,226.63,maize
...
```

### Column Descriptions

| Column | Type | Range | Description |
|--------|------|-------|-------------|
| **N** | Integer | 0-150 | Nitrogen content in kg/ha |
| **P** | Integer | 0-150 | Phosphorus content in kg/ha |
| **K** | Integer | 0-250 | Potassium content in kg/ha |
| **temperature** | Float | -10 to 50 | Temperature in Celsius |
| **humidity** | Float | 0-100 | Relative humidity in percentage |
| **pH** | Float | 0-14 | Soil pH value |
| **rainfall** | Float | 0-500 | Annual rainfall in mm |
| **label** | String | Any | Crop name (must match exactly) |

### Valid Crops

The system supports 22 crops by default:

```
rice, maize, chickpea, kidneybeans, pigeonpeas, mothbeans, mungbean, 
blackgram, lentil, pomegranate, banana, mango, grapes, watermelon, 
muskmelon, apple, orange, papaya, coconut, cotton, sugarcane, tobacco
```

You can add your own crops as long as you have training data for them.

## Dataset Preparation

### 1. Create Your CSV

**Option A: Manual Creation**
```
Create a spreadsheet with sensor readings and crop labels:
- Each row = one soil sample
- Columns = features + label
```

**Option B: From Kaggle**
```
Download from: https://www.kaggle.com/atharvaingle/crop-recommendation-dataset
The dataset has 2200 samples, 22 crops, already in correct format
```

**Option C: Use Sample Data**
```bash
# We provide sample_data.csv with 100 diverse samples
python train.py --csv data/crop_recommendation_sample.csv
```

### 2. Data Quality Checks

The training script automatically:
- âœ… Validates CSV format
- âœ… Removes null values
- âœ… Removes duplicates
- âœ… Removes outliers (values outside valid ranges)
- âœ… Balances dataset (optional)

To view statistics before training:

```python
from recommendation_ai.data_loader import CropDataLoader

loader = CropDataLoader()
df = loader.load('data/crop_data.csv')
loader.print_statistics()
```

### 3. Minimum Dataset Size

Recommended samples per crop:
- **Minimum**: 10 samples per crop
- **Good**: 50+ samples per crop
- **Excellent**: 100+ samples per crop

With 22 crops:
- Minimum total: 220 samples
- Recommended total: 1100+ samples

## Training

### Basic Training

```bash
# Train with sample data
python train.py --csv data/crop_recommendation_sample.csv
```

Output:
```
============================================================
ðŸŒ¾ CROP RECOMMENDATION MODEL TRAINING
============================================================
ðŸ“‚ Loading data from data/crop_recommendation_sample.csv...
âœ… Loaded 130 samples
ðŸ“Š Unique crops: 22
ðŸŒ¾ Crops: apple, banana, blackgram, ...

ðŸ”„ Preprocessing data...
âœ… Features: ['N', 'P', 'K', 'temperature', 'humidity', 'pH', 'rainfall']
âœ… Features shape: (130, 7)
âœ… Labels shape: (130,)
âœ… Train set: 104 samples
âœ… Test set: 26 samples
âœ… Feature scaling: StandardScaler

ðŸ¤– Training Random Forest (n_estimators=100)...
âœ… Model trained successfully

ðŸ“Š Evaluating model...
ðŸ“ˆ Performance Metrics:
   Train Accuracy: 1.0000
   Test Accuracy:  0.9615
   Test F1-Score:  0.9615
   Test Precision: 0.9615
   Test Recall:    0.9615
   CV Score:       0.9615 (+/- 0.0391)

ðŸ’¾ Model saved to: models/crop_recommender.pkl
   Scaler: models/crop_recommender_scaler.pkl
   Encoder: models/crop_recommender_encoder.pkl
   Metadata: models/crop_recommender_metadata.json
```

### Advanced Training

```bash
# Custom hyperparameters
python train.py \
  --csv data/crop_data.csv \
  --model models/custom_model.pkl \
  --n-estimators 200 \
  --test-size 0.15 \
  --random-state 42
```

### Training Options

```bash
python train.py --help

Options:
  --csv CSV_PATH                 Path to CSV file [REQUIRED]
  --model MODEL_PATH             Path to save model (default: models/crop_recommender.pkl)
  --n-estimators N               Number of trees (default: 100)
  --test-size SIZE               Test fraction, 0-1 (default: 0.2)
  --random-state SEED            Random seed (default: 42)
  --no-verbose                   Disable verbose output
```

## Model Files

After training, you'll have 4 files:

```
models/
â”œâ”€â”€ crop_recommender.pkl           # Trained model
â”œâ”€â”€ crop_recommender_scaler.pkl    # Feature scaler
â”œâ”€â”€ crop_recommender_encoder.pkl   # Label encoder
â””â”€â”€ crop_recommender_metadata.json # Training metadata
```

**Metadata Example:**
```json
{
  "feature_columns": ["N", "P", "K", "temperature", "humidity", "pH", "rainfall"],
  "label_column": "label",
  "crop_labels": ["apple", "banana", "blackgram", ...],
  "n_estimators": 100,
  "metrics": {
    "train_accuracy": 0.9615,
    "test_accuracy": 0.9615,
    "test_f1": 0.9615,
    "cv_mean": 0.9615,
    "cv_std": 0.0391
  },
  "training_samples": 104,
  "test_samples": 26
}
```

## Using Your Trained Model

### In Python Code

```python
from recommendation_ai.crop_recommender import CropRecommender

# Load your trained model
recommender = CropRecommender(model_path='models/my_model.pkl')

# Get recommendation
result = recommender.recommend(
    N=90,           # Nitrogen
    P=42,           # Phosphorus
    K=43,           # Potassium
    temperature=23, # Â°C
    humidity=75,    # %
    ph=6.5,
    rainfall=200    # mm
)

print(f"Recommended crop: {result['recommended_crop']}")
print(f"Confidence: {result['confidence']:.2%}")
```

### In Web App

```python
# Add to web_app.py
from recommendation_ai.crop_recommender import CropRecommender

# Initialize with trained model
crop_recommender = CropRecommender(model_path='models/crop_recommender.pkl')

@app.route('/api/recommend-crop', methods=['POST'])
def recommend():
    data = request.json
    result = crop_recommender.recommend(
        data['N'], data['P'], data['K'],
        data['temperature'], data['humidity'],
        data['ph'], data['rainfall']
    )
    return jsonify(result)
```

## Performance Metrics

The training script reports:

### Accuracy Metrics
- **Train Accuracy**: How well model fits training data
- **Test Accuracy**: How well model generalizes to new data
- **Cross-Validation Score**: Average of 5-fold CV

### Classification Metrics
- **Precision**: Of predicted crops, how many are correct
- **Recall**: Of actual crops, how many are found
- **F1-Score**: Harmonic mean of precision and recall

### Benchmarks

**With Sample Data (130 samples):**
```
Test Accuracy: 96.15%
Test F1-Score: 0.9615
CV Score: 0.9615 Â± 0.0391
```

**With Full Kaggle Dataset (2200 samples):**
```
Expected Test Accuracy: 98%+
Expected F1-Score: 0.98+
Expected CV Score: 0.98+
```

## Troubleshooting

### "CSV file not found"
```bash
# Make sure file path is correct
python train.py --csv data/crop_recommendation_sample.csv
```

### "Missing columns: {'N', 'P', 'K'...}"
```bash
# CSV must have exactly these columns:
# N, P, K, temperature, humidity, pH, rainfall, label

# Check your CSV headers match exactly
head -1 data/your_file.csv
```

### "No model to save"
- Make sure training completed successfully
- Check disk space for saving files

### Poor Model Performance

**If accuracy is below 80%:**

1. **Check data quality**
   ```python
   from recommendation_ai.data_loader import CropDataLoader
   loader = CropDataLoader()
   df = loader.load('data/crop_data.csv')
   loader.print_statistics()
   ```

2. **Balance dataset**
   ```python
   loader.balance_dataset(target_samples=50)
   ```

3. **Check for outliers**
   - Values outside valid ranges are automatically removed
   - See VALID_RANGES in data_loader.py

4. **Increase training data**
   - Collect more samples
   - Aim for 100+ samples per crop

5. **Tune hyperparameters**
   ```bash
   python train.py --csv data/crop_data.csv --n-estimators 200 --test-size 0.15
   ```

## Data Augmentation

To increase dataset size without collecting more data:

```python
from recommendation_ai.data_loader import CropDataLoader

loader = CropDataLoader()
df = loader.load('data/crop_data.csv')

# Add slight noise to existing samples
import numpy as np
noise = np.random.normal(0, 0.05, df.shape)
df_augmented = df + noise

# Balance to equal samples per crop
loader.balance_dataset(target_samples=100)
```

## Comparing Models

To compare different trained models:

```python
from recommendation_ai.crop_recommender import CropRecommender
import json

# Load two models
model1 = CropRecommender(model_path='models/model1.pkl')
model2 = CropRecommender(model_path='models/model2.pkl')

# Compare on test data
with open('models/model1_metadata.json') as f:
    meta1 = json.load(f)
    print(f"Model 1 Accuracy: {meta1['metrics']['test_accuracy']:.4f}")

with open('models/model2_metadata.json') as f:
    meta2 = json.load(f)
    print(f"Model 2 Accuracy: {meta2['metrics']['test_accuracy']:.4f}")
```

## Retraining

To update model with new data:

```bash
# Combine old and new data
cat data/old_data.csv data/new_data.csv > data/combined_data.csv

# Train new model
python train.py --csv data/combined_data.csv --model models/retrained_model.pkl
```

## Best Practices

âœ… **Do:**
- Keep 80/20 train/test split for validation
- Use standardized feature scaling (automatic)
- Cross-validate with k-fold (automatic)
- Use 100+ estimators for RandomForest
- Collect balanced data across all crops

âŒ **Don't:**
- Use all data for training (no test set)
- Mix different units/scales
- Have class imbalance >10:1
- Train with <20 samples per crop
- Skip outlier detection

## References

1. **Kaggle Dataset**: https://www.kaggle.com/atharvaingle/crop-recommendation-dataset
2. **Research Paper**: "Harvestify: Agricultural Intelligence For Farming" (ACM INDIA 2021)
3. **Feature Ranges**: Based on Indian agricultural conditions

## Support

For issues or questions:
1. Check TRAINING.md (this file)
2. Review examples.py
3. Check data_loader.py for data utilities
4. Run with `--no-verbose` for cleaner output

---

**Status**: âœ… Ready for Production

**Next Steps**: Train your model â†’ Integrate into web app â†’ Monitor performance
